# Path to training data in a format accepted by pandas (e.g. read_csv, read_hdf)
input:
  path : null

outputs:
  save_test_scores         : true
  save_train_scores        : false
  save_estimators          : false
  save_test_predictions    : false
  save_train_predictions   : false
  save_indices             : false
  save_feature_importances : false

random_seed: 0

preprocess:
  features: null
  target  : target
  index   : null
  # TODO: positive_class : null

model_selector: null

train_test_iterator:
  # Option 1)
  name: train_test_split
  # Option 2) sklearn CrossValidator on all data
  # name: KFold
  # n_splits: 5
  # Option 3) KFold on train set + evaluation
  # TODO: Option X) GridSearchCV on train set + evaluation of best model on test set

train_on_all : false

# Names for splits used in DataFrame columns. 'null' results in simple
# enumeration.
split_names: null

# Collapse predictions for each split into a single column (a.k.a.
# pd.DataFrame.stack) if possible. The common use case would be stacking
# predictions on test folds during cross validation into a single column of
# predictions given that the test folds partition the data. If test sets overlap
# across splits or training predictions are saved out alongside test
# predictions, then stacking is not possible as there are multiple predictions
# per row. In that case, this option is ignored.
stack_split_predictions: true


# TODO: Allow user to specify multiple estimators as a list or even "all Classifiers"
estimator:
  name: ExtraTreesClassifier
  # n_estimators: 10

fit:
  # return_* keywords determined by save_* parameters in outputs section so no
  # need to manually set them here
  scoring:
  - accuracy
  # - neg_log_loss
