# Path to training data in a format accepted by pandas (e.g. read_csv, read_hdf)
input:
  path : null
  # TODO: data, model, predictions, is_test_data

outputs:
  # Output directory for saving outputs
  path : null
  # Overwrite output directory if it exists. Otherwise, ask user for input.
  overwrite : False
  # Create timestamp sub-directory in `path` for saving outputs instead of
  # saving directly to `path`.
  create_subdir : False
  # Output toggles
  save_test_scores         : true
  save_train_scores        : false
  save_estimators          : false
  save_test_predictions    : false
  save_train_predictions   : false
  save_indices             : false
  save_feature_importances : false
  # TODO: visualization outputs
  # save_confusion_matrix_plot : True
  # save_roc_plot : True
  # save_prob_plot: False
  # save_feature_plots: False


random_seed: 0

preprocess:
    features   : null   # X = df[features]
    target     : target # y = df[target]
    index      : null   # df.set_index(index)
    target_map : null   # df[target] = df[target].map(target_map)
    astype     : null   # df = df.astype(astype)
    # TODO: positive_class : null

model_selector: null

train_test_iterator:
    # Option 1)
    name: train_test_split
    # Option 2) sklearn CrossValidator on all data
    # name: KFold
    # n_splits: 5
    # Option 3) KFold on train set + evaluation
    # Option 4) Custom
    # name: skutils.train_test_iterators.AblationSplit
    # kfold_cv:
    #   name:

    # TODO: Option X) GridSearchCV on train set + evaluation of best model on test set

train_on_all : false

# Names for splits used in DataFrame columns. 'null' results in simple
# enumeration.
split_names: null

# Collapse predictions for each split into a single column (a.k.a.
# pd.DataFrame.stack) if possible. The common use case would be stacking
# predictions on test folds during cross validation into a single column of
# predictions given that the test folds partition the data. If test sets overlap
# across splits or training predictions are saved out alongside test
# predictions, then stacking is not possible as there are multiple predictions
# per row. In that case, this option is ignored.
stack_split_predictions: true


# TODO: Allow user to specify multiple estimators as a list or even "all Classifiers"
estimator: null

fit:
    # return_* keywords determined by save_* parameters in outputs section so no
    # need to manually set them here
    scoring: null
    # - accuracy
    # - neg_log_loss

visualization:
  image_ext: .png
  roc:
    # TODO: simplify roc curve plotting functions
    facet_grid: null # kwargs for seaborn.facetgrid
    facet_grid_roc: null # kwargs for _facetgrid_plot_roc_curve
    bootstrap: null # kwargs for roc.bootstrap
    curve: null # kwargs for plot_roc_curve
    scaling:
      tp_count : null
      fp_count : null
  # TODO: Add other sklearn supported visualizations: confusion matrix, learning
  # curve, etc.

logging:
    # kwargs for logging.basicConfig()
    format : '%(levelname)8s | %(module)s :: %(message)s'
    level: INFO
    force: True
    filename : null #run.log
    filemode : w
    # kwargs for logging.dictConfig
    # dict_config: null
    #   Add below to temporarily change properties of a specific logger
    #   disable_existing_loggers: False
    #   loggers:
    #     skutils.prediction:
    #       level : 'DEBUG'
