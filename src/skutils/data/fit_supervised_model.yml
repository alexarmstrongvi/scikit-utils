# Path to training data in a format accepted by pandas (e.g. read_csv, read_hdf)
input:
    path : null

outputs:
    # Path to directory for saving all outputs
    path : null
    # Save all outputs in a timestamped subdirectory of `path`
    timestamp_subdir : false
    # Overwrite
    overwrite : false
    # Toggles for each output
    save_input_configs       : false
    save_final_config        : true
    save_git_diff            : false
    save_test_scores         : true
    save_train_scores        : false
    save_estimators          : false
    save_test_predictions    : false
    save_train_predictions   : false
    save_indices             : false
    save_feature_importances : false

random_seed: 0

preprocess:
    features: null
    target  : target
    index   : null
    # TODO: positive_class : null

model_selector: null

train_test_iterator:
    # Option 1)
    name: train_test_split
    # Option 2) sklearn CrossValidator on all data
    # name: KFold
    # n_splits: 5
    # Option 3) KFold on train set + evaluation
    # Option 4) Custom
    # name: skutils.train_test_iterators.AblationSplit
    # kfold_cv:
    #   name:

    # TODO: Option X) GridSearchCV on train set + evaluation of best model on test set

train_on_all : false

# Names for splits used in DataFrame columns. 'null' results in simple
# enumeration.
split_names: null

# Collapse predictions for each split into a single column (a.k.a.
# pd.DataFrame.stack) if possible. The common use case would be stacking
# predictions on test folds during cross validation into a single column of
# predictions given that the test folds partition the data. If test sets overlap
# across splits or training predictions are saved out alongside test
# predictions, then stacking is not possible as there are multiple predictions
# per row. In that case, this option is ignored.
stack_split_predictions: true


# TODO: Allow user to specify multiple estimators as a list or even "all Classifiers"
estimator: null

fit:
    # return_* keywords determined by save_* parameters in outputs section so no
    # need to manually set them here
    scoring: null
    # - accuracy
    # - neg_log_loss

logging:
    # kwargs for logging.basicConfig()
    format : '%(levelname)8s | %(module)s :: %(message)s'
    level: INFO
    force: True
    filename : null #run.log
    filemode : w
    # kwargs for logging.dictConfig
    # dict_config: null
    #   Add below to temporarily change properties of a specific logger
    #   disable_existing_loggers: False
    #   loggers:
    #     skutils.prediction:
    #       level : 'DEBUG'
